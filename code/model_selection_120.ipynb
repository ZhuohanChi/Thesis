{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cb8e90-e241-4483-9a1b-cd9aa7b5d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # 启用 IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30dbbb2b-0edf-4a11-82c8-aaa888293093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_selection(data: pd.DataFrame, target: str, id_column: str, \n",
    "                        n_splits: int = 5, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform model selection given a dataset using Accuracy, Precision, Recall, and F1-score.\n",
    "\n",
    "    This function:\n",
    "        1. Drops rows with more than 95% missing values.\n",
    "        2. Splits data into features (X) and target (y).\n",
    "        3. Uses MICE (IterativeImputer) with max_iter=20 to impute missing values.\n",
    "        4. Trains multiple classification models using cross-validation.\n",
    "        5. Prints a summary table of Accuracy, Precision, Recall, and F1-score.\n",
    "        6. Returns the best model based on F1-score.\n",
    "\n",
    "    :param data: Input DataFrame containing features and target.\n",
    "    :param target: The name of the target column.\n",
    "    :param id_column: The name of the ID column which is not used for training.\n",
    "    :param n_splits: Number of folds for Stratified K-Fold cross validation.\n",
    "    :param random_state: Random seed for reproducibility.\n",
    "    :return: The best performing model (already fit on the entire dataset).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Drop rows with missing ratio > 0.95\n",
    "    row_missing_ratio = data.isnull().sum(axis=1) / data.shape[1]\n",
    "    data = data.loc[row_missing_ratio <= 0.95].copy()\n",
    "\n",
    "    # Separate target and ID from the features\n",
    "    X = data.drop(columns=[target, id_column])\n",
    "    y = data[target]\n",
    "\n",
    "    # Step 2: Apply MICE for missing data imputation\n",
    "    imputer = IterativeImputer(max_iter=12, random_state=random_state)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    # Define classification models\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"Multilayer Perceptron\": MLPClassifier(max_iter=1000),\n",
    "        \"SVM\": SVC(probability=True),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"LightGBM\": LGBMClassifier(),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(),\n",
    "        \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "        \"Bagging Classifier\": BaggingClassifier(),\n",
    "        \"HistGradientBoosting\": HistGradientBoostingClassifier(),\n",
    "    }\n",
    "\n",
    "    # Step 3: Cross-validation for each model\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_f1_score = 0.0\n",
    "`\n",
    "    # Define scoring metrics\n",
    "    scoring_metrics = {\n",
    "        \"accuracy\": make_scorer(accuracy_score),\n",
    "        \"precision\": make_scorer(precision_score, average=\"weighted\"),\n",
    "        \"recall\": make_scorer(recall_score, average=\"weighted\"),\n",
    "        \"f1_score\": make_scorer(f1_score, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        accuracy = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"accuracy\"]))\n",
    "        precision = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"precision\"]))\n",
    "        recall = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"recall\"]))\n",
    "        f1 = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"f1_score\"]))\n",
    "\n",
    "        results.append([model_name, accuracy, precision, recall, f1])\n",
    "\n",
    "        # Keep track of the best model based on F1-score\n",
    "        if f1 > best_f1_score:\n",
    "            best_f1_score = f1\n",
    "            best_model = model_name\n",
    "\n",
    "    # Create a results DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "    \n",
    "    # Print the results in table form\n",
    "    print(\"\\n===== Model Evaluation Results =====\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Print the best model\n",
    "    print(f\"\\n===== Best Model (Based on F1-score) =====\")\n",
    "    print(f\"Best Model: {best_model}\")\n",
    "    print(f\"Best F1-score: {best_f1_score:.4f}\")\n",
    "\n",
    "    # Fit the best model on the entire dataset\n",
    "    final_model = models[best_model]\n",
    "    final_model.fit(X_imputed, y)\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538b3b49-3670-4d31-8379-e4da377da7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data: pd.DataFrame, target: str, id_column: str):\n",
    "    \"\"\"\n",
    "    Main function to load data and perform model selection.\n",
    "    \"\"\"\n",
    "    best_model = run_model_selection(data, target, id_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53514be3-098b-4761-8f91-5fc54c93ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:22:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21231\n",
      "[LightGBM] [Info] Number of data points in the train set: 5224, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.614089 -> initscore=0.464532\n",
      "[LightGBM] [Info] Start training from score 0.464532\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21226\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21214\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21201\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21231\n",
      "[LightGBM] [Info] Number of data points in the train set: 5224, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.614089 -> initscore=0.464532\n",
      "[LightGBM] [Info] Start training from score 0.464532\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21226\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21214\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21201\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21231\n",
      "[LightGBM] [Info] Number of data points in the train set: 5224, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.614089 -> initscore=0.464532\n",
      "[LightGBM] [Info] Start training from score 0.464532\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21226\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21214\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21201\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2016\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21231\n",
      "[LightGBM] [Info] Number of data points in the train set: 5224, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.614089 -> initscore=0.464532\n",
      "[LightGBM] [Info] Start training from score 0.464532\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21226\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21214\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21201\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "[LightGBM] [Info] Number of positive: 3208, number of negative: 2017\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21212\n",
      "[LightGBM] [Info] Number of data points in the train set: 5225, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.613971 -> initscore=0.464036\n",
      "[LightGBM] [Info] Start training from score 0.464036\n",
      "\n",
      "===== Model Evaluation Results =====\n",
      "                Model  Accuracy  Precision   Recall  F1-score\n",
      "        Decision Tree  0.700507   0.703542 0.700201  0.704238\n",
      "  Logistic Regression  0.767416   0.764961 0.767416  0.763801\n",
      "          Naive Bayes  0.699889   0.746076 0.699889  0.702332\n",
      "                  KNN  0.732354   0.732003 0.732354  0.732147\n",
      "Multilayer Perceptron  0.715212   0.737090 0.706926  0.689973\n",
      "                  SVM  0.744296   0.745692 0.744296  0.731241\n",
      "    Gradient Boosting  0.800183   0.798610 0.800183  0.798331\n",
      "             AdaBoost  0.785024   0.783319 0.785177  0.783263\n",
      "              XGBoost  0.779819   0.777798 0.779819  0.777966\n",
      "        Random Forest  0.787781   0.786582 0.787933  0.791122\n",
      "             LightGBM  0.791149   0.789250 0.791149  0.789323\n",
      "          Extra Trees  0.786861   0.781287 0.784106  0.777603\n",
      "             CatBoost  0.799264   0.797551 0.799264  0.797533\n",
      "   Bagging Classifier  0.753330   0.759692 0.757464  0.750644\n",
      " HistGradientBoosting  0.791762   0.789807 0.791762  0.789804\n",
      "\n",
      "===== Best Model (Based on F1-score) =====\n",
      "Best Model: Gradient Boosting\n",
      "Best F1-score: 0.7983\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the NICU dataset\n",
    "    nicu_120 = pd.read_csv(r\"../data/final/nicu_120.csv\")\n",
    "\n",
    "    # Define target column and ID column\n",
    "    target_column = \"is_infected\"  # Target variable\n",
    "    id_column = \"SUBJECT_ID\"       # ID column\n",
    "\n",
    "    # Run the main function\n",
    "    main(data=nicu_120, target=target_column, id_column=id_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
