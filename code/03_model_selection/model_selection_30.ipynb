{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cb8e90-e241-4483-9a1b-cd9aa7b5d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # 启用 IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30dbbb2b-0edf-4a11-82c8-aaa888293093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_selection(data: pd.DataFrame, target: str, id_column: str, \n",
    "                        n_splits: int = 5, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform model selection given a dataset using Accuracy, Precision, Recall, and F1-score.\n",
    "\n",
    "    This function:\n",
    "        1. Drops rows with more than 95% missing values.\n",
    "        2. Splits data into features (X) and target (y).\n",
    "        3. Uses MICE (IterativeImputer) with max_iter=20 to impute missing values.\n",
    "        4. Trains multiple classification models using cross-validation.\n",
    "        5. Prints a summary table of Accuracy, Precision, Recall, and F1-score.\n",
    "        6. Returns the best model based on F1-score.\n",
    "\n",
    "    :param data: Input DataFrame containing features and target.\n",
    "    :param target: The name of the target column.\n",
    "    :param id_column: The name of the ID column which is not used for training.\n",
    "    :param n_splits: Number of folds for Stratified K-Fold cross validation.\n",
    "    :param random_state: Random seed for reproducibility.\n",
    "    :return: The best performing model (already fit on the entire dataset).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Drop rows with missing ratio > 0.95\n",
    "    row_missing_ratio = data.isnull().sum(axis=1) / data.shape[1]\n",
    "    data = data.loc[row_missing_ratio <= 0.95].copy()\n",
    "\n",
    "    # Separate target and ID from the features\n",
    "    X = data.drop(columns=[target, id_column])\n",
    "    y = data[target]\n",
    "\n",
    "    # Step 2: Apply MICE for missing data imputation\n",
    "    imputer = IterativeImputer(max_iter=12, random_state=random_state)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    # Define classification models\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"Multilayer Perceptron\": MLPClassifier(max_iter=1000),\n",
    "        \"SVM\": SVC(probability=True),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"LightGBM\": LGBMClassifier(),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(),\n",
    "        \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "        \"Bagging Classifier\": BaggingClassifier(),\n",
    "        \"HistGradientBoosting\": HistGradientBoostingClassifier(),\n",
    "    }\n",
    "\n",
    "    # Step 3: Cross-validation for each model\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_f1_score = 0.0\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring_metrics = {\n",
    "        \"accuracy\": make_scorer(accuracy_score),\n",
    "        \"precision\": make_scorer(precision_score, average=\"weighted\"),\n",
    "        \"recall\": make_scorer(recall_score, average=\"weighted\"),\n",
    "        \"f1_score\": make_scorer(f1_score, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        accuracy = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"accuracy\"]))\n",
    "        precision = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"precision\"]))\n",
    "        recall = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"recall\"]))\n",
    "        f1 = np.mean(cross_val_score(model, X_imputed, y, cv=skf, scoring=scoring_metrics[\"f1_score\"]))\n",
    "\n",
    "        results.append([model_name, accuracy, precision, recall, f1])\n",
    "\n",
    "        # Keep track of the best model based on F1-score\n",
    "        if f1 > best_f1_score:\n",
    "            best_f1_score = f1\n",
    "            best_model = model_name\n",
    "\n",
    "    # Create a results DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "    \n",
    "    # Print the results in table form\n",
    "    print(\"\\n===== Model Evaluation Results =====\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    # Print the best model\n",
    "    print(f\"\\n===== Best Model (Based on F1-score) =====\")\n",
    "    print(f\"Best Model: {best_model}\")\n",
    "    print(f\"Best F1-score: {best_f1_score:.4f}\")\n",
    "\n",
    "    # Fit the best model on the entire dataset\n",
    "    final_model = models[best_model]\n",
    "    final_model.fit(X_imputed, y)\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538b3b49-3670-4d31-8379-e4da377da7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data: pd.DataFrame, target: str, id_column: str):\n",
    "    \"\"\"\n",
    "    Main function to load data and perform model selection.\n",
    "    \"\"\"\n",
    "    best_model = run_model_selection(data, target, id_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53514be3-098b-4761-8f91-5fc54c93ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\littl\\anaconda3\\envs\\thesis\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [19:17:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1072\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21353\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696661 -> initscore=0.831448\n",
      "[LightGBM] [Info] Start training from score 0.831448\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21350\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21340\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21356\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21354\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1072\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21353\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696661 -> initscore=0.831448\n",
      "[LightGBM] [Info] Start training from score 0.831448\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21350\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21340\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21356\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21354\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1072\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21353\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696661 -> initscore=0.831448\n",
      "[LightGBM] [Info] Start training from score 0.831448\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21350\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21340\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21356\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21354\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1072\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21353\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696661 -> initscore=0.831448\n",
      "[LightGBM] [Info] Start training from score 0.831448\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21350\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2461, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21340\n",
      "[LightGBM] [Info] Number of data points in the train set: 3534, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696378 -> initscore=0.830109\n",
      "[LightGBM] [Info] Start training from score 0.830109\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21356\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "[LightGBM] [Info] Number of positive: 2462, number of negative: 1073\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21354\n",
      "[LightGBM] [Info] Number of data points in the train set: 3535, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.696464 -> initscore=0.830516\n",
      "[LightGBM] [Info] Start training from score 0.830516\n",
      "\n",
      "===== Model Evaluation Results =====\n",
      "                Model  Accuracy  Precision   Recall  F1-score\n",
      "        Decision Tree  0.677458   0.690211 0.683344  0.684782\n",
      "  Logistic Regression  0.745136   0.734095 0.745136  0.711721\n",
      "          Naive Bayes  0.721371   0.710474 0.721371  0.713527\n",
      "                  KNN  0.719337   0.708167 0.719337  0.711369\n",
      "Multilayer Perceptron  0.706860   0.718558 0.739479  0.674379\n",
      "                  SVM  0.731328   0.724223 0.731328  0.675947\n",
      "    Gradient Boosting  0.777960   0.770663 0.778639  0.762841\n",
      "             AdaBoost  0.757137   0.743946 0.757137  0.742026\n",
      "              XGBoost  0.755325   0.744124 0.755325  0.745632\n",
      "        Random Forest  0.761437   0.751907 0.763699  0.750517\n",
      "             LightGBM  0.764605   0.754282 0.764605  0.754441\n",
      "          Extra Trees  0.749213   0.734443 0.751701  0.737599\n",
      "             CatBoost  0.776831   0.767883 0.776831  0.763427\n",
      "   Bagging Classifier  0.729973   0.738514 0.728613  0.725706\n",
      " HistGradientBoosting  0.764378   0.753834 0.764378  0.753941\n",
      "\n",
      "===== Best Model (Based on F1-score) =====\n",
      "Best Model: CatBoost\n",
      "Best F1-score: 0.7634\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the NICU dataset\n",
    "    nicu_30 = pd.read_csv(r\"../data/final/nicu_30.csv\")\n",
    "\n",
    "    # Define target column and ID column\n",
    "    target_column = \"is_infected\"  # Target variable\n",
    "    id_column = \"SUBJECT_ID\"       # ID column\n",
    "\n",
    "    # Run the main function\n",
    "    main(data=nicu_30, target=target_column, id_column=id_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
