{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab77736-6699-447d-8810-da50a8c891a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eedf17a-17cd-488a-ac47-b315658c2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_gradient_boosting(data: pd.DataFrame,\n",
    "                                             target: str,\n",
    "                                             drop_threshold: float = 0.75,\n",
    "                                             max_iter_mice: int = 12,\n",
    "                                             n_splits: int = 5,\n",
    "                                             random_state: int = 42,\n",
    "                                             gb_params: dict = None):\n",
    "    \"\"\"\n",
    "    Perform feature selection based on Gradient Boosting feature importances.\n",
    "    \n",
    "    Process:\n",
    "        1. Drop rows with missing ratio > drop_threshold.\n",
    "        2. MICE imputation for missing values.\n",
    "        3. Train a Gradient Boosting model using all features to obtain feature importances.\n",
    "        4. Sort features by importance in descending order.\n",
    "        5. For top X% of features (X from 10% to 50% with step=5%), re-train the model\n",
    "           and evaluate performance via cross-validation.\n",
    "        6. Print results for each subset of features.\n",
    "\n",
    "    :param data: Input dataframe (includes features + target column).\n",
    "    :param target: The name of the target column.\n",
    "    :param drop_threshold: If row's missing ratio > drop_threshold, drop it. Default=0.95.\n",
    "    :param max_iter_mice: Number of MICE iteration, default=12.\n",
    "    :param n_splits: Number of folds for cross-validation, default=5.\n",
    "    :param random_state: Seed for reproducibility, default=42.\n",
    "    :param gb_params: A dict of hyperparameters for GradientBoostingClassifier, \n",
    "                      e.g., {\"n_estimators\":300, \"learning_rate\":0.05, \"max_depth\":4}.\n",
    "                      If None, we use a default config.\n",
    "    :return: None (prints out the performance results of different feature subsets).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Drop rows with too many missing values\n",
    "    row_missing_ratio = data.isnull().sum(axis=1) / data.shape[1]\n",
    "    data_cleaned = data.loc[row_missing_ratio <= drop_threshold].copy()\n",
    "    \n",
    "    # 2. Separate target and features\n",
    "    y = data_cleaned[target]\n",
    "    X = data_cleaned.drop(columns=[target])\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # 3. MICE imputation\n",
    "    imputer = IterativeImputer(max_iter=max_iter_mice, random_state=random_state)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # 4. Train a Gradient Boosting model to get feature importances\n",
    "    if gb_params is None:\n",
    "        gb_params = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 3,\n",
    "            \"random_state\": random_state\n",
    "        }\n",
    "    gb_clf = GradientBoostingClassifier(**gb_params)\n",
    "    gb_clf.fit(X_imputed, y)\n",
    "\n",
    "    # Get feature importances and sort\n",
    "    importances = gb_clf.feature_importances_\n",
    "    feat_imp_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # For convenience, get sorted feature names\n",
    "    sorted_features = feat_imp_df[\"Feature\"].tolist()\n",
    "\n",
    "    # 5. For coverage from 10% to 50% in increments of 5%, select top features and re-train\n",
    "    n_total_features = len(feature_names)\n",
    "    coverage_list = range(10, 81, 5)  # 10%, 15%, 20%, ..., 50%\n",
    "\n",
    "    results = []\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    for coverage in coverage_list:\n",
    "        # Compute how many features to keep\n",
    "        n_keep = max(1, int(np.ceil(n_total_features * (coverage / 100.0))))\n",
    "        \n",
    "        # Select top n_keep features\n",
    "        selected_feats = sorted_features[:n_keep]\n",
    "        \n",
    "        # Build a subset of X_imputed with only those features\n",
    "        selected_indices = [feature_names.get_loc(feat) for feat in selected_feats]\n",
    "        X_sub = X_imputed[:, selected_indices]\n",
    "\n",
    "        # Re-train Gradient Boosting on these top features using cross-validation\n",
    "        model = GradientBoostingClassifier(**gb_params)\n",
    "        scores = cross_val_score(model, X_sub, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "        \n",
    "        mean_acc = scores.mean()\n",
    "        std_acc = scores.std()\n",
    "\n",
    "        results.append({\n",
    "            \"Coverage(%)\": coverage,\n",
    "            \"Num_Features\": n_keep,\n",
    "            \"Mean_Accuracy\": mean_acc,\n",
    "            \"Std_Accuracy\": std_acc\n",
    "        })\n",
    "\n",
    "    # 6. Print feature importance + results\n",
    "    print(\"\\n=== Feature Importances (All Features) ===\")\n",
    "    print(feat_imp_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n=== Feature Selection Results (Top X% of Features) ===\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a1a541-d8db-4066-934f-8eda388b4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate feature_selection_with_gradient_boosting usage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example: read data (please modify the path as necessary)\n",
    "    data = pd.read_csv(r\"../data/final/nicu_30.csv\")\n",
    "\n",
    "    # Suppose your target column name is 'is_infected'\n",
    "    target_column = \"is_infected\"\n",
    "\n",
    "    # Example gradient boosting hyperparameters from prior knowledge or tuning\n",
    "    gb_params = {\n",
    "        \"n_estimators\": 50,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"max_depth\": 5,\n",
    "        \"subsample\": 0.6, \n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    # Call the feature selection function\n",
    "    feature_selection_with_gradient_boosting(\n",
    "        data=data,\n",
    "        target=target_column,\n",
    "        drop_threshold=0.75,\n",
    "        max_iter_mice=12,\n",
    "        n_splits=5,\n",
    "        random_state=42,\n",
    "        gb_params=gb_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d38a975-4fa4-4ed3-a879-b8c6cead0fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importances (All Features) ===\n",
      "                     Feature  Importance\n",
      "          Lymphocytes_min_30    0.108206\n",
      "          Lymphocytes_max_30    0.085953\n",
      "         Lymphocytes_mean_30    0.075930\n",
      "                 SaO2_min_30    0.060169\n",
      "      BP Cuff [Mean]_mean_30    0.028227\n",
      "  BP Cuff [Systolic]_mean_30    0.024705\n",
      "                 SaO2_max_30    0.024296\n",
      "     Temp Axilary [F]_max_30    0.021776\n",
      "                SaO2_mean_30    0.021507\n",
      "           Heart Rate_max_30    0.019217\n",
      "                  SUBJECT_ID    0.018593\n",
      "      Temp/Iso/Warmer_min_30    0.017325\n",
      "           Glucometer_min_30    0.016725\n",
      "   BP Cuff [Systolic]_min_30    0.016677\n",
      "            Resp Rate_max_30    0.015651\n",
      "                 MCV_mean_30    0.015099\n",
      "      Red Blood Cells_max_30    0.015062\n",
      "         Neutrophils_mean_30    0.014619\n",
      "      Temp/Iso/Warmer_max_30    0.014610\n",
      "           Glucometer_max_30    0.014503\n",
      "    White Blood Cells_min_30    0.014376\n",
      "      Red Blood Cells_min_30    0.013511\n",
      "       Platelet Count_max_30    0.013301\n",
      "           Heart Rate_min_30    0.013108\n",
      "            Resp Rate_min_30    0.013015\n",
      "          Heart Rate_mean_30    0.012115\n",
      "     Temp/Iso/Warmer_mean_30    0.011160\n",
      "   BP Cuff [Systolic]_max_30    0.011075\n",
      "     Red Blood Cells_mean_30    0.010555\n",
      "       BP Cuff [Mean]_min_30    0.010052\n",
      "   White Blood Cells_mean_30    0.009919\n",
      "      Platelet Count_mean_30    0.009209\n",
      " BP Cuff [Diastolic]_mean_30    0.009158\n",
      "    White Blood Cells_max_30    0.009148\n",
      "                  RDW_max_30    0.009003\n",
      "                  MCV_max_30    0.008941\n",
      "                  MCV_min_30    0.008932\n",
      "                 RDW_mean_30    0.008789\n",
      "                  RDW_min_30    0.008412\n",
      "        Temp Skin [C]_min_30    0.008388\n",
      "       BP Cuff [Mean]_max_30    0.008341\n",
      "          Glucometer_mean_30    0.008250\n",
      "          Neutrophils_max_30    0.007823\n",
      "           Resp Rate_mean_30    0.007795\n",
      "       Platelet Count_min_30    0.007608\n",
      "  BP Cuff [Diastolic]_min_30    0.007237\n",
      "    Temp Axilary [F]_mean_30    0.006805\n",
      "       Temp Skin [C]_mean_30    0.006545\n",
      "           Hematocrit_max_30    0.005842\n",
      "     Temp Axilary [F]_min_30    0.005118\n",
      "        Temp Skin [C]_max_30    0.005038\n",
      "                 MCHC_max_30    0.004803\n",
      "          Neutrophils_min_30    0.004581\n",
      "                Bands_min_30    0.004467\n",
      "                 MCHC_min_30    0.004430\n",
      "                  MCH_min_30    0.004337\n",
      "          Hematocrit_mean_30    0.004319\n",
      "                 MCH_mean_30    0.004278\n",
      "           Hemoglobin_min_30    0.004030\n",
      "            Monocytes_min_30    0.003683\n",
      "  BP Cuff [Diastolic]_max_30    0.003562\n",
      "                MCHC_mean_30    0.003497\n",
      "               Bands_mean_30    0.003159\n",
      "           Hemoglobin_max_30    0.003127\n",
      "          Hemoglobin_mean_30    0.003121\n",
      "          Eosinophils_min_30    0.003119\n",
      "         Eosinophils_mean_30    0.002961\n",
      "                Bands_max_30    0.002926\n",
      "           Monocytes_mean_30    0.002711\n",
      " Atypical Lymphocytes_min_30    0.002590\n",
      "            Monocytes_max_30    0.002442\n",
      "                  MCH_max_30    0.001404\n",
      "          Eosinophils_max_30    0.001393\n",
      "           Hematocrit_min_30    0.001358\n",
      " Atypical Lymphocytes_max_30    0.001230\n",
      "           Myelocytes_min_30    0.001183\n",
      "      Metamyelocytes_mean_30    0.001013\n",
      "            Basophils_max_30    0.000718\n",
      "Atypical Lymphocytes_mean_30    0.000634\n",
      "          Myelocytes_mean_30    0.000519\n",
      "       Metamyelocytes_min_30    0.000391\n",
      "       Metamyelocytes_max_30    0.000297\n",
      "            Basophils_min_30    0.000294\n",
      "           Basophils_mean_30    0.000004\n",
      "           Myelocytes_max_30    0.000000\n",
      "\n",
      "=== Feature Selection Results (Top X% of Features) ===\n",
      " Coverage(%)  Num_Features  Mean_Accuracy  Std_Accuracy\n",
      "          10             9       0.772965      0.011852\n",
      "          15            13       0.780061      0.019168\n",
      "          20            17       0.781124      0.014670\n",
      "          25            22       0.784671      0.013427\n",
      "          30            26       0.787508      0.014108\n",
      "          35            30       0.786090      0.017338\n",
      "          40            34       0.787157      0.013431\n",
      "          45            39       0.786091      0.014309\n",
      "          50            43       0.787152      0.016982\n",
      "          55            47       0.786799      0.013925\n",
      "          60            51       0.784318      0.015195\n",
      "          65            56       0.787864      0.011656\n",
      "          70            60       0.787866      0.011607\n",
      "          75            64       0.786799      0.009852\n",
      "          80            68       0.784672      0.012380\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000cc5-50e4-4f90-8974-d887cbb4c367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
